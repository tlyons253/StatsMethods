---
title: "Important considerations for developing unreplicated surveys"
author: "Tim Lyons"
date: "2023-06-02"
output:
  pdf_document: default
  toc: true
subtitle: 'or\: You know what grinds my gears'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message=FALSE,
                      dpi=300)
library(tidyverse)
```
# Introduction
## Structured but unreplicated: The August Roadside Survey
## What next?

Unstructed surveys are hard to use because of the limited ways to relate the observed count  with the true, underlying population.

In surveys like our August Roadside Survey, despite the data we publish, we are not actually estimating the population size or density. The survey measures the amount of change, which we assume is reflected in our roadside counts from one year to the next. Because we know how much effort we are putting into our survey (the same routes), the effort is constant and so any change we see is mostly attributable to a change in the underlying population.

Contrast that with unstructured brood surveys for turkeys. When folks submit opportunistic observations it's not always clear what that number represents. Are individuals reporting birds seen while driving, or just walking through their backyard? That might not be as much of a problem in any one year to develop an index, but what happens when we are trying to track changes from one year to the next? Opportunistic surveys can vary greatly from one year to the next in terms of their participation and effort. Therefore, any change observed could be attributable to the number of observers, how much time they spent observing, where, etc. Even individual observers may not devote the some amount of effort from one year to the next. This makes any observed changes in the index very suspect-how much of the change is due to observation effort/behavior vs a true population change. Requiring observers to report all this additional information is often unsuccessful or makes participation overly burdensome. 

Typically these sorts of surveys can be written statistically as:

$$
Y_i=qN_i
$$
where $Y_i$ is the observed index in year $i$, $q$ is a "proportionality constant" that encompasses detection, availability, and survey effort, and $N_i$ is the true population value for that year.

Although $Y_i$ isn't the true population count, it's still helpful to use to monitor changes in trends. For example, you can still estimate $\lambda$ the growth rate between two years as:
$$\frac{Y_2}{Y_1}=\lambda_1$$
This is overly optimistic, as we know $q$ is going to vary from one year to the next just due to simple things like general randomness-Like how we know if we run the same ARS route, under the same conditions, 5 times in one year we can still get 5 different counts.

In this case, the estimated change from one year to the next isn't terribly accurate, but it is unbiased. Over time (think 10 years) that "randomness" will average out and the long term trend, will be a good approximation for the "true" trend.

As a way to demonstrate this, I have a simulation where $q$ is allowed to vary a little bit each year. Going back to the model for our observed counts:
$$
Y_i=q_iN_i
$$

We can think of $q$ now being year-specific $q_i$ and having it's own little submodel. We use the logit function here because $q_i$ is a probability (0,1) and its easier to add covariates to a probability when you use the the logit link/ function:
$$
logit(q_i)=\mu+e_i
$$
$$
e_i\sim N(0,\sigma^2)
$$
This equation just means that the proportion index in any given year $q_i$, is a combination of some "average proportion" $\mu$ that does not vary across years and a noise/ random error term $e_i$ that does vary among years.

The next equation just shows the randomness of the noise/ error is represented as coming from a normal distribution, centered at 0 with some amount of variance.

In practical terms, this means that instead of the proportion index being a fixed value, it now has a distribution, like below.
```{r}

#set.seed(25)
my.sigma<-0.5
logit.mu<-log(0.1/(0.9))
logit.x<-logit.mu+rnorm(100000,0,my.sigma)

plot(density(exp(logit.x)/(1+exp(logit.x))),
     main=bquote(atop('Distribution of observed proportion indexes',sigma^2== .(my.sigma,))),
     ylab='',
     xlab='Observed proportion')



```

To demonstrate this, here is a 10 year simulation with an average growth rate of 3% per year, or about 30% growth over 10 years.

```{r, include=FALSE}
N<-c(100,rep(NA,9))

for (i in 2:10){
  N[i]<-N[i-1]*1.03
}

Y<-seq(1,10,1)

dat<-data.frame(N=N,Y=Y)

### observation model
#set.seed(23)
error<-rnorm(10,0,0.5)
mu<-0.1
logit.mu<-log(0.1/(0.9))
logit.q<-logit.mu+error

q<-exp(logit.q)/(1+exp(logit.q))


data.frame(dat,q)%>%
  mutate(obs=q*N,
         true.lambda=N/lag(N),
         obs.lambda=obs/lag(obs),
         net.true=prod(true.lambda,na.rm=TRUE),
         net.obs=prod(obs.lambda,na.rm=TRUE))->dat2


```


```{r}
ggplot2::ggplot(dat2,aes(Y,N))+
  geom_point(size=2,color='black',shape='circle')+
  geom_line(aes(Y,N,group=1),color='black',linetype=2,lwd=0.15)+
  geom_line(aes(Y,obs),color='gray25',linetype=1,lwd=0.15)+
  scale_x_continuous(breaks= seq(1,20,1))+
   theme_classic()+
  xlab('\n Year')+
  ylab('Pop. size (N) \n')+
  theme(axis.text.x=element_text(color='black',size=7,face='bold'),
        axis.text.y=element_text(color='black',size=7,face='bold'),
        axis.title.y=element_text(size=9,face='bold'),
        axis.title.x=element_text(size=9,face='bold'),
        axis.line.y=element_line(color='black',size=0.5),
        axis.ticks.y=element_line(color='black',size=0.5),
        axis.ticks.length=unit(1.5,'mm'))->truth.plot

truth.plot
#######
# ggplot2::ggplot(dat,aes(Y,true.lambda))+
#   geom_point(size=2,color='black',shape='circle')+
#   geom_line(aes(Y,true.lambda,group=1),color='black',linetype=2,lwd=0.15)+
#   geom_line(aes(Y,obs.lambda),color='gray25',linetype=1,lwd=0.15)+
#   scale_x_continuous(breaks= seq(1,20,1))+
#    theme_classic()+
#   xlab('\n Year')+
#   ylab(bquote(lambda))+
#   theme(axis.text.x=element_text(color='black',size=7,face='bold'),
#         axis.text.y=element_text(color='black',size=7,face='bold'),
#         axis.title.y=element_text(size=9,face='bold'),
#         axis.title.x=element_text(size=9,face='bold'),
#         axis.line.y=element_line(color='black',size=0.5),
#         axis.ticks.y=element_line(color='black',size=0.5),
#         axis.ticks.length=unit(1.5,'mm'))

```

The darker line is the "true" population size while the gray line below is the index. In this case, the net population change over the 10 yr, based on the index, is `r round(unique(dat2$net.obs),digits=2)`. This is what we would expect to see if, for instance, we tried to make any conclusions about pheasant population trends from just a single ARS survey route. It's maybe not ideal.

The value in indices, like the ARS, come from the fact we have many opportunities to "observe" the annual change. In the ARS, this comes in the form of having 150+ different survey routes. If we average the annual change we observe along each route, it should be close to the "true" change of the overall population.

Coming back to the above example, lets assume that instead of a single observation each year, we had 10 different observations (10 different "routes" if you will). 
```{r}

#need to fix this, should be closer to true value when calculating mean growth later on
error<-matrix(rnorm(100,0,0.5), ncol=10)
mu<-0.1
logit.mu<-log(0.1/(0.9))
logit.q<-logit.mu+error

q<-exp(logit.q)/(1+exp(logit.q))




rownames(q)<-paste0('Y_',seq(1,10,1))
colnames(q)<-paste0('S_',seq(1,10,1))


data.frame(dat,q)%>%
  mutate(across(starts_with('S'),~N*.x))%>%
  pivot_longer(cols=S_1:S_10,names_to='grp',
               values_to = 'obs')->dat.10


ggplot2::ggplot(dat.10,aes(Y,N))+
  geom_point(size=2,color='black',shape='circle')+
  geom_line(aes(Y,N,group=1),color='black',linetype=2,lwd=0.15)+
  geom_line(aes(Y,obs,group=grp),color='gray25',linetype=1,lwd=0.15)+
  scale_x_continuous(breaks= seq(1,10,1))+
   theme_classic()+
  xlab('\n Year')+
  ylab('Pop. size (N) \n')+
  theme(axis.text.x=element_text(color='black',size=7,face='bold'),
        axis.text.y=element_text(color='black',size=7,face='bold'),
        axis.title.y=element_text(size=9,face='bold'),
        axis.title.x=element_text(size=9,face='bold'),
        axis.line.y=element_line(color='black',size=0.5),
        axis.ticks.y=element_line(color='black',size=0.5),
        axis.ticks.length=unit(1.5,'mm'))

```

```{r}
dat.10%>%
  group_by(grp)%>%
  mutate(obs.lambda=obs/lag(obs))%>%
  summarize(net.lambda=prod(obs.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.lambda),digits = 2))->better



```

Now, with 10 observation-series of a population, our 10 year growth rate averaged across all 10 series is `r better$mu[1]`. This still isn't the 1.30 net growth of the real population, but it's maybe a bit closer.

The reality is, it takes a lot more 10 series/routes/what have you to get accurate estimates of trends. The above is still an overoptimistic view of these sorts of surveys. Population growth is never constant as it was simulated here-we might expect an average annual growth of 1.03, but it won't be that every year. Below is a graph of what the distribution of annual growth rates looks like, given an mean 1.03 and SD of 0.3.
```{r}

my.sigma=0.3

plot(density(rnorm(100000,1.03,my.sigma)),
     main=bquote(atop('Distribution of annual growth rates',
                     mu==1.03~' '~sigma== .(my.sigma,))),
     ylab='',
     xlab=bquote("Annual growth rate" ~'('~lambda~')'))




```

And here is what we might estimate from a fixed-effort survey, like the ARS,  with 20-150 series/routes, but treating annual population growth ($\lambda$) as being 1.03 on average, with a standard deviation of $\sigma$ of 0.3. We also have random error in our proportion we observe, and variation in the starting population size being surveyed by each series/route.

```{r,include=FALSE, eval=FALSE}
#20 series
df.obs20<-data.frame(mu=c(),q.10=c(),q.90=c())


for(k in 1:1000){
n.obs<-20

lam.mat<-matrix(rnorm(n.obs*9,1.03,0.3),nrow=n.obs,ncol=9)

error<-matrix(rnorm(n.obs*10,0,0.5),nrow=n.obs,ncol=10)
mu<-0.2
logit.mu<-log(0.2/(0.8))
logit.q<-logit.mu+error

q<-exp(logit.q)/(1+exp(logit.q))

N<-matrix(NA,nrow=n.obs,ncol=10)
N[,1]<-rpois(n.obs,50)

obs.n<-matrix(NA,nrow=n.obs,ncol=10)

for(i in 1:n.obs){
  for(t in 2:10){
    N[i,t]<-rpois(1,N[i,t-1]*lam.mat[i,t-1])
  }
}

for(i in 1:n.obs){
  for(t in 1:10){
    obs.n[i,t]<-rbinom(1,size=N[i,t],prob=q[i,t])
  }
}

#obs.n<-N*q

rownames(N)<-paste0('s_',seq(1,n.obs,1))
colnames(N)<-paste0('Y_',seq(1,10,1))


data.frame(N)%>%
  rowid_to_column()%>%
  pivot_longer(cols=starts_with('Y'),
               names_to='year',
               values_to='N')%>%
  group_by(rowid)%>%
  mutate(N.lambda=N/lag(N))%>%
  summarize(net.lambda=prod(N.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.lambda),digits = 2),
            q.10=quantile(net.lambda,0.1),
            q.90=quantile(net.lambda,0.9))->N.20

### now for observed data 
rownames(obs.n)<-paste0('s_',seq(1,n.obs,1))
colnames(obs.n)<-paste0('Y_',seq(1,10,1))


data.frame(obs.n)%>%
  rowid_to_column()%>%
  pivot_longer(cols=starts_with('Y'),
               names_to='year',
               values_to='obs')%>%
  group_by(rowid)%>%
mutate(obs=ifelse(obs==0,1,obs))%>% # trying to deal with a 0 to >1 count with a series between years
  
  mutate(obs.lambda=obs/lag(obs))%>%
  summarize(net.obslambda=prod(obs.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.obslambda),digits = 2),
            q.10=quantile(net.obslambda,0.1),
            q.90=quantile(net.obslambda,0.9))->obs.20
  
 df.obs20<-rbind(df.obs20,obs.20)
 }
df.obs20<-mutate(df.obs20,N=rep(20,nrow(df.obs20)))



#########     100 series     #############




df.obs100<-data.frame(mu=c(),q.10=c(),q.90=c())

for (k in 1:1000){

n.obs<-100

lam.mat<-matrix(rnorm(n.obs*9,1.03,0.3),nrow=n.obs,ncol=9)

error<-matrix(rnorm(n.obs*10,0,0.5),nrow=n.obs,ncol=10)
mu<-0.2
logit.mu<-log(0.2/(0.8))
logit.q<-logit.mu+error

q<-exp(logit.q)/(1+exp(logit.q))

N<-matrix(NA,nrow=n.obs,ncol=10)
N[,1]<-rpois(n.obs,50)

obs.n<-matrix(NA,nrow=n.obs,ncol=10)

for(i in 1:n.obs){
  for(t in 2:10){
    N[i,t]<-rpois(1,N[i,t-1]*lam.mat[i,t-1])
  }
}

# for(i in 1:n.obs){
#   for(t in 1:10){
#     obs.n[i,t]<-rbinom(1,size=N[i,t],prob=q[i,t])
#   }
# }


obs.n<-N*q

rownames(N)<-paste0('s_',seq(1,n.obs,1))
colnames(N)<-paste0('Y_',seq(1,10,1))


data.frame(N)%>%
  rowid_to_column()%>%
  pivot_longer(cols=starts_with('Y'),
               names_to='year',
               values_to='N')%>%
  group_by(rowid)%>%
  mutate(N.lambda=N/lag(N))%>%
  summarize(net.lambda=prod(N.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.lambda),digits = 2),
            q.10=quantile(net.lambda,0.1),
            q.90=quantile(net.lambda,0.9))->N.100

### now for observed data 
rownames(obs.n)<-paste0('s_',seq(1,n.obs,1))
colnames(obs.n)<-paste0('Y_',seq(1,10,1))


data.frame(obs.n)%>%
  rowid_to_column()%>%
  pivot_longer(cols=starts_with('Y'),
               names_to='year',
               values_to='obs')%>%
  group_by(rowid)%>%
mutate(obs=ifelse(obs==0,0.0000001,obs))%>% # trying to deal with a 0 to >1 count with a series between years
  
  mutate(obs.lambda=obs/lag(obs))%>%
  summarize(net.obslambda=prod(obs.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.obslambda),digits = 2),
            q.10=quantile(net.obslambda,0.1),
            q.90=quantile(net.obslambda,0.9))->obs.100
  
 df.obs100<-rbind(df.obs100,obs.100) 
}


df.obs100<-mutate(df.obs100,N=rep(100,nrow(df.obs100)))
#######     N 50 series   #########



df.obs50<-data.frame(mu=c(),q.10=c(),q.90=c())
  
for (k in 1:1000){
  
n.obs<-50

lam.mat<-matrix(rnorm(n.obs*9,1.03,0.3),nrow=n.obs,ncol=9)

error<-matrix(rnorm(n.obs*10,0,0.5),nrow=n.obs,ncol=10)
mu<-0.2
logit.mu<-log(0.2/(0.8))
logit.q<-logit.mu+error

q<-exp(logit.q)/(1+exp(logit.q))

N<-matrix(NA,nrow=n.obs,ncol=10)
N[,1]<-rpois(n.obs,50)

obs.n<-matrix(NA,nrow=n.obs,ncol=10)

for(i in 1:n.obs){
  for(t in 2:10){
    N[i,t]<-rpois(1,N[i,t-1]*lam.mat[i,t-1])
  }
}

# for(i in 1:n.obs){
#   for(t in 1:10){
#     obs.n[i,t]<-rbinom(1,size=N[i,t],prob=q[i,t])
#   }
# }


obs.n<-N*q


rownames(N)<-paste0('s_',seq(1,n.obs,1))
colnames(N)<-paste0('Y_',seq(1,10,1))


data.frame(N)%>%
  rowid_to_column()%>%
  pivot_longer(cols=starts_with('Y'),
               names_to='year',
               values_to='N')%>%
  group_by(rowid)%>%
  mutate(N.lambda=N/lag(N))%>%
  summarize(net.lambda=prod(N.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.lambda),digits = 2),
            q.10=quantile(net.lambda,0.1),
            q.90=quantile(net.lambda,0.9))->N.50

### now for observed data 
rownames(obs.n)<-paste0('s_',seq(1,n.obs,1))
colnames(obs.n)<-paste0('Y_',seq(1,10,1))


data.frame(obs.n)%>%
  rowid_to_column()%>%
  pivot_longer(cols=starts_with('Y'),
               names_to='year',
               values_to='obs')%>%
  group_by(rowid)%>%
mutate(obs=ifelse(obs==0,0.0000001,obs))%>% # trying to deal with a 0 to >1 count with a series between years
  
  mutate(obs.lambda=obs/lag(obs))%>%
  summarize(net.obslambda=prod(obs.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.obslambda),digits = 2),
            q.10=quantile(net.obslambda,0.1),
            q.90=quantile(net.obslambda,0.9))->obs.50

df.obs50<-rbind(df.obs50,obs.50)
}
df.obs50<-mutate(df.obs50,N=rep(50,nrow(df.obs50)))

################        150 series        ###########################

df.obs150<-data.frame(mu=c(),q.10=c(),q.90=c())

for (k in 1:1000){
n.obs<-150

lam.mat<-matrix(rnorm(n.obs*9,1.03,0.3),nrow=n.obs,ncol=9)

error<-matrix(rnorm(n.obs*10,0,0.5),nrow=n.obs,ncol=10)
mu<-0.2
logit.mu<-log(0.2/(0.8))
logit.q<-logit.mu+error

q<-exp(logit.q)/(1+exp(logit.q))

N<-matrix(NA,nrow=n.obs,ncol=10)
N[,1]<-rpois(n.obs,50)

obs.n<-matrix(NA,nrow=n.obs,ncol=10)

for(i in 1:n.obs){
  for(t in 2:10){
    N[i,t]<-rpois(1,N[i,t-1]*lam.mat[i,t-1])
  }
}

# for(i in 1:n.obs){
#   for(t in 1:10){
#     obs.n[i,t]<-rbinom(1,size=N[i,t],prob=q[i,t])
#   }
# }

obs.n<-N*q


rownames(N)<-paste0('s_',seq(1,n.obs,1))
colnames(N)<-paste0('Y_',seq(1,10,1))


data.frame(N)%>%
  rowid_to_column()%>%
  pivot_longer(cols=starts_with('Y'),
               names_to='year',
               values_to='N')%>%
  group_by(rowid)%>%
  mutate(N.lambda=N/lag(N))%>%
  summarize(net.lambda=prod(N.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.lambda),digits = 2),
            q.10=quantile(net.lambda,0.1),
            q.90=quantile(net.lambda,0.9))->N.150

### now for observed data 
rownames(obs.n)<-paste0('s_',seq(1,n.obs,1))
colnames(obs.n)<-paste0('Y_',seq(1,10,1))


data.frame(obs.n)%>%
  rowid_to_column()%>%
  pivot_longer(cols=starts_with('Y'),
               names_to='year',
               values_to='obs')%>%
  group_by(rowid)%>%
mutate(obs=ifelse(obs==0,0.0000001,obs))%>% # trying to deal with a 0 to >1 count with a series between years
  
  mutate(obs.lambda=obs/lag(obs))%>%
  summarize(net.obslambda=prod(obs.lambda,na.rm=TRUE))%>%
  summarize(mu=round(mean(net.obslambda),digits = 2),
            q.10=quantile(net.obslambda,0.1),
            q.90=quantile(net.obslambda,0.9))->obs.150
 

df.obs150<-rbind(df.obs150,obs.150)
 
}

df.obs150<-mutate(df.obs150, N=rep(150,nrow(df.obs150)))

```




```{r, eval=FALSE, include=FALSE}
# Code to produce table 1
bind_rows(df.obs20,df.obs50,df.obs100,df.obs150)%>%
  group_by(N)%>%
  summarize(med=mean(mu),mu.10=quantile(mu,0.10),
            mu.90=quantile(mu,0.9))
  relocate(`Sample Size`)%>%
  kableExtra::kbl(caption= '<b>Tab1. </b>Estimated average net growth from surveys of varying sample sizes (routes)',
                     align = 'c',
                  col.names = c('Sample Size',"Net $\\lambda$", "10% Quantile", "90% quantile"), escape=FALSE)%>%
     kableExtra::column_spec(1,bold=T)%>%
  kableExtra::row_spec(0,bold=T,underline = F,color='black')%>%
     kableExtra::collapse_rows(columns=c(1,2,3,4),
                               valign='middle')%>%
     kableExtra::kable_classic(full_width=TRUE,html_font = 'Arial')

```
Even 150 observations/samples/routes might not be enough to provide a reasonable estimate of the trend at a 10 year time scale. And remember, this is akin to the ARS, where the same routes are sampled each year. If we were to consider varying the effort (some time series only sampled sporadically, only at the beginning or end of the 10-yr simulated time series, etc.) you might imagine the range of values and the estimated average would be worse.